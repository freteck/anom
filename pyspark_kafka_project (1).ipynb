{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "updated - run this one on localhost:8888\n",
    "\n",
    "### Anomaly Detection in Server Logs\n",
    "#### CS 5614: Big Data Engineering\n",
    "#### By: Vanessa Eichensehr and Bradley Freedman\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Project Objective:**  \n",
    "\n",
    "Build a machine learning-based model that detects anomalies on a high volume high velocity log base. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.9/dist-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0c62bc65-e452-426c-b8db-d80c808eb75f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.4.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.4-3 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 381ms :: artifacts dl 23ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.4-3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0c62bc65-e452-426c-b8db-d80c808eb75f\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/7ms)\n",
      "25/05/03 21:51:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-kafka-streaming\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0\"). \\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** \n",
    "Create a streaming DataFrame in Spark that reads data from a Kafka topic named \"topic_test\" and starts\n",
    "processing from the beginning of the topic's log using the earliest available offset. Use kafka:9093 as the bootstrap\n",
    "server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_streamed_raw = (spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"kafka:9093\").option(\"subscribe\", \"topic_test\").load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# convert byte stream to string\n",
    "df_streamed_kv = (df_streamed_raw\n",
    "    .withColumn(\"key\", df_streamed_raw[\"key\"].cast(StringType()))\n",
    "    .withColumn(\"value\", df_streamed_raw[\"value\"].cast(StringType())))\n",
    "\n",
    "test_query = (df_streamed_kv \n",
    "              .writeStream \\\n",
    "              .format(\"memory\") # output to memory \\\n",
    "              .outputMode(\"update\") # only write updated rows to the sink \\\n",
    "              .queryName(\"test_query_table\")  # Name of the in memory table \\\n",
    "              .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If all goes well, the following cell should display a table populated with values being streamed from you Kafka producer. NOTE: If you recently ran the producer, it may take a while before the table is populated. Keep rerunning the cell to check for updates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from test_query_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following cells contain code that take the streamed dataframe and formats it properly into a table. If any of the given cells fails, there might be a formatting issue with one of your previous solutions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import StructType, StructField, BooleanType, LongType, IntegerType\n",
    "\n",
    "event_schema = StructType([\n",
    "    StructField(\"ip_address\", StringType()),\n",
    "    StructField(\"date_time\", StringType()),\n",
    "    StructField(\"request_type\", StringType()),\n",
    "    StructField(\"request_arg\", StringType()),\n",
    "    StructField(\"status_code\", StringType()),\n",
    "    StructField(\"response_size\", StringType()),\n",
    "    StructField(\"referrer\", StringType()),\n",
    "    StructField(\"user_agent\", StringType())\n",
    "])\n",
    "\n",
    "# Parse the events from JSON format\n",
    "df_parsed = (df_streamed_kv\n",
    "           # Sets schema for event data\n",
    "           .withColumn(\"value\", from_json(\"value\", event_schema))\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_formatted = (df_parsed.select(\n",
    "    col(\"key\").alias(\"event_key\")\n",
    "    ,col(\"topic\").alias(\"event_topic\")\n",
    "    ,col(\"timestamp\").alias(\"event_timestamp\")\n",
    "    ,col(\"value.ip_address\").alias(\"ip_address\")\n",
    "    ,col(\"value.date_time\").alias(\"date_time\")\n",
    "    ,col(\"value.request_type\").alias(\"request_type\")\n",
    "    ,col(\"value.request_arg\").alias(\"request_arg\")\n",
    "    ,col(\"value.status_code\").alias(\"status_code\")\n",
    "    ,col(\"value.response_size\").cast(IntegerType()).alias(\"response_size\")\n",
    "    ,col(\"value.referrer\").alias(\"referrer\")\n",
    "    ,col(\"value.user_agent\").alias(\"user_agent\")\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the parsed data to console\n",
    "query = (df_formatted.writeStream.format(\"console\").outputMode(\"append\").trigger(processingTime='5 seconds').start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the name of active streams (This may be useful during debugging)\n",
    "for s in spark.streams.active:\n",
    "    print(f\"ID:{s.id} | NAME:{s.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best Guess at Next Steps**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load labeled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+------+--------------------+------+------+--------------------+--------------------+---------+-------------+\n",
      "|           ip|                time|method|                path|status|  size|            referrer|          user_agent|anomalous|     category|\n",
      "+-------------+--------------------+------+--------------------+------+------+--------------------+--------------------+---------+-------------+\n",
      "| 54.36.149.86|2019-01-22 03:59:...|   GET|/image/8739/speci...|   200|179495|https://www.zanbi...|Mozilla/5.0 (X11;...|        0|not anomalous|\n",
      "| 5.62.206.249|2019-01-22 03:59:...|   GET|      /settings/logo|   302|159434|https://www.zanbi...|Mozilla/5.0 (Wind...|        0|not anomalous|\n",
      "| 54.36.149.16|2019-01-22 03:59:...|   GET|/product/4031/20/...|   302| 74066|https://www.zanbi...|Dalvik/2.1.0 (Lin...|        0|not anomalous|\n",
      "|66.249.66.195|2019-01-22 03:59:...|   GET|/image/65274/prod...|   302| 66812|https://www-zanbi...|Mozilla/5.0 (Linu...|        0|not anomalous|\n",
      "| 95.81.118.56|2019-01-22 03:59:...|   GET|/product/30275/59...|   301| 65979|https://www.zanbi...|Mozilla/5.0 (Wind...|        0|not anomalous|\n",
      "+-------------+--------------------+------+--------------------+------+------+--------------------+--------------------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_df = spark.read.csv(\"/data/synthetic_with_anomalies_NEW.csv\", header=True, inferSchema=True)\n",
    "training_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (2.0.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-process and engineer features (might need to encode or use string indexer, depending on model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+------+--------------------+------+------+--------------------+--------------------+---------+-------------+----------+------+----------+--------+------------+--------------+--------------------+-----+\n",
      "|           ip|                time|method|                path|status|  size|            referrer|          user_agent|anomalous|     category| unix_time|ip_idx|method_idx|path_idx|referrer_idx|user_agent_idx|            features|label|\n",
      "+-------------+--------------------+------+--------------------+------+------+--------------------+--------------------+---------+-------------+----------+------+----------+--------+------------+--------------+--------------------+-----+\n",
      "| 54.36.149.86|2019-01-22 03:59:...|   GET|/image/8739/speci...|   200|179495|https://www.zanbi...|Mozilla/5.0 (X11;...|        0|not anomalous|1548129551|  79.0|       0.0|    59.0|        11.0|          25.0|[200.0,179495.0,1...|  0.0|\n",
      "| 5.62.206.249|2019-01-22 03:59:...|   GET|      /settings/logo|   302|159434|https://www.zanbi...|Mozilla/5.0 (Wind...|        0|not anomalous|1548129551|   3.0|       0.0|   736.0|        19.0|          38.0|[302.0,159434.0,1...|  0.0|\n",
      "| 54.36.149.16|2019-01-22 03:59:...|   GET|/product/4031/20/...|   302| 74066|https://www.zanbi...|Dalvik/2.1.0 (Lin...|        0|not anomalous|1548129551|  36.0|       0.0|   383.0|        34.0|          23.0|[302.0,74066.0,1....|  0.0|\n",
      "|66.249.66.195|2019-01-22 03:59:...|   GET|/image/65274/prod...|   302| 66812|https://www-zanbi...|Mozilla/5.0 (Linu...|        0|not anomalous|1548129552|  45.0|       0.0|   272.0|        49.0|          37.0|[302.0,66812.0,1....|  0.0|\n",
      "| 95.81.118.56|2019-01-22 03:59:...|   GET|/product/30275/59...|   301| 65979|https://www.zanbi...|Mozilla/5.0 (Wind...|        0|not anomalous|1548129552|  98.0|       0.0|   380.0|         0.0|          10.0|[301.0,65979.0,1....|  0.0|\n",
      "+-------------+--------------------+------+--------------------+------+------+--------------------+--------------------+---------+-------------+----------+------+----------+--------+------------+--------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------------------------------------------------------+\n",
      "|features                                               |\n",
      "+-------------------------------------------------------+\n",
      "|[200.0,179495.0,1.548129551E9,79.0,0.0,59.0,11.0,25.0] |\n",
      "|[302.0,159434.0,1.548129551E9,3.0,0.0,736.0,19.0,38.0] |\n",
      "|[302.0,74066.0,1.548129551E9,36.0,0.0,383.0,34.0,23.0] |\n",
      "|[302.0,66812.0,1.548129552E9,45.0,0.0,272.0,49.0,37.0] |\n",
      "|[301.0,65979.0,1.548129552E9,98.0,0.0,380.0,0.0,10.0]  |\n",
      "|[301.0,176909.0,1.548129552E9,67.0,0.0,57.0,10.0,16.0] |\n",
      "|[304.0,92224.0,1.548129552E9,84.0,0.0,562.0,40.0,18.0] |\n",
      "|[302.0,53925.0,1.548129553E9,19.0,0.0,143.0,14.0,4.0]  |\n",
      "|[304.0,11151.0,1.548129553E9,52.0,0.0,239.0,1.0,20.0]  |\n",
      "|[200.0,181467.0,1.548129554E9,75.0,0.0,803.0,40.0,13.0]|\n",
      "|[200.0,174015.0,1.548129554E9,27.0,0.0,114.0,33.0,17.0]|\n",
      "|[304.0,123224.0,1.548129554E9,74.0,0.0,118.0,31.0,28.0]|\n",
      "|[304.0,22350.0,1.548129554E9,23.0,0.0,423.0,50.0,27.0] |\n",
      "|[404.0,58693.0,1.548129555E9,40.0,0.0,574.0,44.0,23.0] |\n",
      "|[302.0,129259.0,1.548129555E9,92.0,0.0,615.0,33.0,2.0] |\n",
      "|[200.0,129469.0,1.548129555E9,6.0,0.0,24.0,42.0,35.0]  |\n",
      "|[301.0,106665.0,1.548129556E9,40.0,0.0,575.0,7.0,17.0] |\n",
      "|[404.0,82569.0,1.548129556E9,64.0,0.0,585.0,35.0,45.0] |\n",
      "|[302.0,92755.0,1.548129556E9,97.0,0.0,139.0,23.0,40.0] |\n",
      "|[404.0,19443.0,1.548129557E9,24.0,0.0,320.0,38.0,15.0] |\n",
      "+-------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "\n",
    "training_df = training_df.withColumn(\n",
    "    \"unix_time\", \n",
    "    unix_timestamp(\"time\", \"yyyy-MM-dd HH:mm:ss.SSS\")\n",
    ")\n",
    "\n",
    "# Update VectorAssembler to include it\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"status\", \"size\", \"unix_time\"] + [col + \"_idx\" for col in [\"ip\", \"method\", \"path\", \"referrer\", \"user_agent\"]],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# note that REQUEST_TYPE = method\n",
    "# note that REQUEST ARGUMENT = path \n",
    "\n",
    "# If doing classification, include label as well (anomalous or category)\n",
    "label_indexer = StringIndexer(inputCol=\"anomalous\", outputCol=\"label\")  # for binary classification\n",
    "# or: StringIndexer(inputCol=\"category\", outputCol=\"label\") for multi-class\n",
    "\n",
    "# Combine steps\n",
    "pipeline = Pipeline(stages=indexers + [assembler, label_indexer])\n",
    "model = pipeline.fit(training_df)  # only if you're training\n",
    "log_transformed = model.transform(training_df)\n",
    "log_transformed.show(5)\n",
    "log_transformed.select(\"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into test and train sets\n",
    "\n",
    "Train the ML model(s) -- binary and multi-class?\n",
    "\n",
    "save the pipeline and model \n",
    "\n",
    "there is a way to apply the ML model on a stream of data and output to console\n",
    "\n",
    "might need to broadcast this data elsewhere so can show a UI.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = log_transformed.randomSplit([0.8, 0.2], seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    maxBins=1024  # or even 2048 if needed\n",
    ")\n",
    "rf_model = rf.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = rf_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.938\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"AUC: {auc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+\n",
      "|label|prediction|            features|\n",
      "+-----+----------+--------------------+\n",
      "|  0.0|       0.0|[302.0,160997.0,1...|\n",
      "|  0.0|       0.0|[301.0,11625.0,1....|\n",
      "|  0.0|       0.0|[200.0,7899.0,1.5...|\n",
      "|  0.0|       0.0|[304.0,45571.0,1....|\n",
      "|  0.0|       0.0|[302.0,11337.0,1....|\n",
      "|  0.0|       0.0|[302.0,157474.0,1...|\n",
      "|  0.0|       0.0|[302.0,132633.0,1...|\n",
      "|  0.0|       0.0|[200.0,118578.0,1...|\n",
      "|  0.0|       0.0|[404.0,133588.0,1...|\n",
      "|  0.0|       0.0|[200.0,4355.0,1.5...|\n",
      "|  0.0|       0.0|[304.0,68827.0,1....|\n",
      "|  0.0|       0.0|[200.0,118139.0,1...|\n",
      "|  0.0|       0.0|[301.0,59802.0,1....|\n",
      "|  0.0|       0.0|[302.0,84416.0,1....|\n",
      "|  0.0|       0.0|[304.0,160726.0,1...|\n",
      "|  0.0|       0.0|[200.0,42863.0,1....|\n",
      "|  0.0|       0.0|[404.0,132667.0,1...|\n",
      "|  0.0|       0.0|[404.0,146539.0,1...|\n",
      "|  0.0|       0.0|[200.0,151087.0,1...|\n",
      "|  0.0|       0.0|[302.0,32516.0,1....|\n",
      "+-----+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"label\", \"prediction\", \"features\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
