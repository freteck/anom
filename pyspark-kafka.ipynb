{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Assignment 4: PySpark Structured Streaming Using Kafka Source**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-kafka-streaming\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0\"). \\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ==== Q2 ===="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q2.1:** All your code for 2.1 should be in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer to 2.1\n",
    "df_streamed_raw = (spark\n",
    "  .readStream\n",
    "  # Add your code here\n",
    "  .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# convert byte stream to string\n",
    "df_streamed_kv = (df_streamed_raw\n",
    "    .withColumn(\"key\", df_streamed_raw[\"key\"].cast(StringType()))\n",
    "    .withColumn(\"value\", df_streamed_raw[\"value\"].cast(StringType())))\n",
    "\n",
    "test_query = (df_streamed_kv \n",
    "              .writeStream \\\n",
    "              .format(\"memory\") # output to memory \\\n",
    "              .outputMode(\"update\") # only write updated rows to the sink \\\n",
    "              .queryName(\"test_query_table\")  # Name of the in memory table \\\n",
    "              .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If all goes well, the following cell should display a table populated with values being streamed from you Kafka producer. NOTE: If you recently ran the producer, it may take a while before the table is populated. Keep rerunning the cell to check for updates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from test_query_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query.stop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following cells contain code that take the streamed dataframe and formats it properly into a table. If any of the given cells fails, there might be a formatting issue with one of your previous solutions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import StructType, StructField, BooleanType, LongType, IntegerType\n",
    "\n",
    "event_schema = StructType([\n",
    "    StructField(\"ip_address\", StringType()),\n",
    "    StructField(\"date_time\", StringType()),\n",
    "    StructField(\"request_type\", StringType()),\n",
    "    StructField(\"request_arg\", StringType()),\n",
    "    StructField(\"status_code\", StringType()),\n",
    "    StructField(\"response_size\", StringType()),\n",
    "    StructField(\"referrer\", StringType()),\n",
    "    StructField(\"user_agent\", StringType())\n",
    "])\n",
    "\n",
    "# Parse the events from JSON format\n",
    "df_parsed = (df_streamed_kv\n",
    "           # Sets schema for event data\n",
    "           .withColumn(\"value\", from_json(\"value\", event_schema))\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_formatted = (df_parsed.select(\n",
    "    col(\"key\").alias(\"event_key\")\n",
    "    ,col(\"topic\").alias(\"event_topic\")\n",
    "    ,col(\"timestamp\").alias(\"event_timestamp\")\n",
    "    ,col(\"value.ip_address\").alias(\"ip_address\")\n",
    "    ,col(\"value.date_time\").alias(\"date_time\")\n",
    "    ,col(\"value.request_type\").alias(\"request_type\")\n",
    "    ,col(\"value.request_arg\").alias(\"request_arg\")\n",
    "    ,col(\"value.status_code\").alias(\"status_code\")\n",
    "    ,col(\"value.response_size\").cast(IntegerType()).alias(\"response_size\")\n",
    "    ,col(\"value.referrer\").alias(\"referrer\")\n",
    "    ,col(\"value.user_agent\").alias(\"user_agent\")\n",
    "))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q2.2:** All your code for 2.2 should be in the following cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to 2.2\n",
    "query = (df_formatted\n",
    " .writeStream\n",
    " # Add your code here\n",
    " .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the name of active streams (This may be useful during debugging)\n",
    "for s in spark.streams.active:\n",
    "    print(f\"ID:{s.id} | NAME:{s.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ==== Q3 ===="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q3.1:** All your code for 3.1 should be in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q3.2:** All your code for 3.2 should be in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q3.3:** All your code for 3.3 should be in the following cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q3.4:** All your code for 3.4 should be in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
